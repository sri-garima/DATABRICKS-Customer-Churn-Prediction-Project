# DATABRICKS-Customer-Churn-Prediction-Project
This project is done under Databricks-AI challenge organized by Codebasics,Indian Data Club and sponsored by Databricks and the day is 24 january 2026.

# ğŸ›’ Customer Churn Prediction & Retention Analytics Platform

## ğŸ“Œ Project Overview

Customer churn is one of the biggest challenges for e-commerce businesses. Acquiring new customers is significantly more expensive than retaining existing ones. This project builds an **end-to-end, production-grade churn prediction platform** using **Databricks**, following modern data engineering and analytics best practices.

The solution ingests raw transactional data, transforms it using a **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**, engineers customer behavior features (RFM), trains a machine learning model to predict churn probability, and surfaces actionable business insights through SQL analytics and dashboards.

This project is designed as a **portfolio-ready Databricks capstone**, demonstrating real-world data engineering, analytics, and ML skills.

---

## ğŸ¯ Business Problem

E-commerce platforms often lose customers silently when they become inactive. Without early identification, businesses face:

* Revenue loss
* Increased marketing costs
* Reduced customer lifetime value

**Goal:**

* Predict which customers are likely to churn
* Identify high-risk customers early
* Estimate revenue at risk
* Enable data-driven retention strategies

---

## ğŸ§  Solution Approach

### Key Idea

Customer churn can be predicted using **behavioral patterns derived from transaction data**. Even with a single transactional table, meaningful customer features can be engineered.

We use **RFM Analysis**:

* **Recency** â€“ How recently a customer made a purchase
* **Frequency** â€“ How often a customer purchases
* **Monetary** â€“ How much a customer spends

These features are used to train a machine learning model that outputs **churn probability per customer**.

---

## ğŸ—ï¸ Architecture

```
Raw CSV (Online Retail Dataset)
        â†“
Bronze Layer (Delta Lake)
- Raw transactional data
        â†“
Silver Layer
- Cleaned transactions
- Customer-level aggregation
- RFM features
        â†“
Gold Layer
- Churn labels
- Churn predictions
- Revenue at risk
        â†“
Consumption Layer
- SQL Dashboards
- MLflow Models
- Business KPIs
```

---

## ğŸ§± Data Architecture (Medallion)

### ğŸ¥‰ Bronze Layer

* Raw ingestion of online retail transactions
* No transformations applied
* Stored as Delta tables for auditability

**Table:**

* `bronze.retail_raw`

---

### ğŸ¥ˆ Silver Layer

* Data cleaning and standardization
* Removal of invalid records (null customers, cancelled invoices)
* Feature engineering

**Tables:**

* `silver.customer_orders_clean`
* `silver.customer_rfm`

**Features Engineered:**

* Recency (days since last purchase)
* Frequency (number of invoices)
* Monetary (total spend)
* Average order value
* Active months

---

### ğŸ¥‡ Gold Layer

* Business-ready and ML-ready datasets
* Churn labels and predictions

**Tables:**

* `gold.customer_churn_label`
* `gold.churn_predictions`
* `gold.revenue_at_risk`

**Churn Definition:**

```
If last_purchase_date > 180 days ago â†’ churn = 1
Else â†’ churn = 0
```

---

## ğŸ¤– Machine Learning

* **Model Type:** Logistic Regression (baseline), Random Forest (optional)
* **Input Features:** RFM metrics and engagement features
* **Output:** Churn probability (0â€“1)

### MLflow

* Experiment tracking
* Parameter & metric logging
* Model registration

---

## ğŸ“Š Analytics & Dashboards

Key insights generated:

* Overall churn rate
* High-risk customer segments
* Revenue at risk due to churn
* Churn trends over time
* Country-level churn distribution

Dashboards are built using **Databricks SQL** on Gold tables.

---

## âš™ï¸ Orchestration

The entire pipeline is automated using **Databricks Jobs**:

1. Bronze ingestion
2. Silver transformation
3. Gold aggregation
4. ML model training
5. Dashboard refresh

This ensures reproducibility and production readiness.

---

## ğŸ” Governance & Security

* Unity Catalog used for data governance
* Layer-wise access control
* Schema-based permissions

---

## ğŸ› ï¸ Tech Stack

* Databricks
* Apache Spark (PySpark & SQL)
* Delta Lake
* MLflow
* Unity Catalog
* Databricks Workflows & SQL

---

## ğŸ“‚ Dataset

**Source:** Online Retail Dataset (Kaggle)

**Core Columns:**

* InvoiceNo
* StockCode
* Description
* Quantity
* InvoiceDate
* UnitPrice
* CustomerID
* Country

---

## ğŸš€ Outcomes & Business Value

* Early identification of at-risk customers
* Targeted retention strategies
* Reduced revenue loss
* Scalable and production-ready churn prediction pipeline

---

## ğŸ“Œ Future Enhancements

* Real-time churn scoring using streaming data
* Customer segmentation with clustering
* Automated retention recommendations
* Advanced models (XGBoost)

---

## ğŸ‘©â€ğŸ’» Author

Garima Srivastava

---

â­ If you found this project useful, feel free to star the repository!

